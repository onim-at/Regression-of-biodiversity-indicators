<<<<<<< HEAD
{"cells":[{"cell_type":"markdown","metadata":{"id":"Mh4zRTz38Eeo"},"source":["# Regression using satellite images"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":615,"status":"ok","timestamp":1610576026692,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"FhAhO6Ul8Ee-","outputId":"9f8ede64-cde4-4fa9-e91f-1a63b94bf0ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["import sklearn.preprocessing as preprocessing\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import pandas as pd\n","from sklearn.metrics import r2_score\n","from tqdm.notebook import tqdm\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"GFH3Szjt8EfD"},"source":["## train_model"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":865,"status":"ok","timestamp":1610576026947,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"WgDVMpx_8EfG"},"outputs":[],"source":["def train_model(train_dataloader, val_dataloader, model, optimizer, criterion,  out_scaler, feature_scaler, epochs=20, print_step=1000, val_step=10):\n","  best_score = 0.0\n","  best_model = copy.deepcopy(model)\n","  for epoch in tqdm(range(epochs)): \n","    running_loss = 0.0\n","    \n","    for i, sample_batched in enumerate(train_dataloader):\n","      expected_outputs = out_scaler.transform(sample_batched['out']).to(device)\n","      features = feature_scaler.transform(sample_batched['features'])\n","      # zero the parameter gradients\n","      optimizer.zero_grad()\n","      # forward + backward + optimize\n","      outputs = model(sample_batched['image'].to(device), features.to(device))\n","      loss = criterion(outputs, expected_outputs)\n","      loss.backward()\n","      optimizer.step()\n","      # print statistics\n","      running_loss += loss.item()\n","      if i % print_step == print_step-1:    # print every 500 mini-batches\n","          print('[%d, %5d] loss: %.3f' %\n","                (epoch + 1, i + 1, running_loss / print_step))\n","          running_loss = 0.0\n","    \n","    if (epoch % val_step == val_step-1):\n","        current_score = test_model(val_dataloader, model)\n","        train_score = test_model(train_dataloader, model)\n","        print(f'Train score at epoch {epoch}: {train_score}')\n","        print(f'Validation score at epoch {epoch}: {current_score}')\n","        if (current_score \u003e best_score):\n","          print(\"Storing current model\")\n","          best_score = current_score\n","          best_model = copy.deepcopy(model)\n","\n","  return model, best_model, best_score\n","  print('Finished Training')"]},{"cell_type":"markdown","metadata":{"id":"Pl3RRnc08EfI"},"source":["## test_model"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":852,"status":"ok","timestamp":1610576026949,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"nU4O2EMx8EfN"},"outputs":[],"source":["def test_model(dataloader, model):\n","    with torch.no_grad():\n","        expected_outputs = []\n","        outputs = []\n","        for sample_batched in iter(dataloader):\n","            expected_outputs += out_scaler.transform(sample_batched['out']).to(device)\n","            features = feature_scaler.transform(sample_batched['features'])\n","\n","            outputs +=  model(sample_batched['image'].to(device), features.to(device))\n","    #print(expected_outputs)\n","    #print(outputs)\n","    score = r2_score(outputs, expected_outputs)\n","    return score"]},{"cell_type":"markdown","metadata":{"id":"e3aKTobn8EfS"},"source":["## BiodiversityDataset"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":848,"status":"ok","timestamp":1610576026954,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"cKPllExj8EfT"},"outputs":[],"source":["class BiodiversityDataset(Dataset):\n","\n","    def __init__(self, csv_file, img_dir, image_transform=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with features.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a image.\n","        \"\"\"\n","        print(\"Loading features...\")\n","        self.features = pd.read_csv(csv_file)\n","        print(\"Done\")\n","        self.img_dir = img_dir\n","        self.image_transform = image_transform\n","        self.images = {}\n","        print(\"Loading images...\")\n","        for index, row in self.features.iterrows():\n","          lon = row.longitude\n","          lat = row.latitude\n","          self.images[str([lon, lat])] = 0\n","          img_name = 'it_' + str(lat) + \"_\" + str(lon) + \".jpg\"\n","          img_path = os.path.join(self.img_dir, img_name)\n","          image = Image.open(img_path).convert('RGB')\n","          if self.image_transform:\n","            image = self.image_transform(image).to(device)\n","          self.images[str([lon, lat])] = image\n","        print(\"Done\")\n","\n","\n","        \n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        lon = self.features.loc[idx].longitude\n","        lat = self.features.loc[idx].latitude\n","        # img_name = 'it_' + str(lat) + \"_\" + str(lon) + \".jpg\"\n","        # img_path = os.path.join(self.img_dir, img_name)\n","        \n","        # image = Image.open(img_path)\n","        # if self.image_transform:\n","        #     image = self.image_transform(image)\n","        image = self.images[str([lon, lat])]\n","        out = torch.from_numpy(np.array([self.features.loc[idx]['habitat_richness']])).detach().clone().reshape([-1])\n","        features = self.features.drop(columns=['habitat_richness', 'longitude', 'latitude']).loc[idx].values\n","        features = torch.from_numpy(features.astype('float').reshape(-1, 46)).detach().clone().reshape([-1])\n","        sample = {'image': image, 'features': features, 'out': out}\n","\n","        return sample\n","    \n","    def get_feature(self):\n","         return torch.from_numpy(self.features.drop(columns=['habitat_richness', 'longitude', 'latitude']).values).detach().clone()\n","    \n","    def get_out(self):\n","        return torch.from_numpy(self.features['habitat_richness'].values).detach().clone()\n","    "]},{"cell_type":"markdown","metadata":{"id":"OjsmQ8yU8EfU"},"source":["## FeatureImageNet"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1160,"status":"ok","timestamp":1610576027276,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"_ga_cZXr8EfV"},"outputs":[],"source":["class FeatureImageNet(nn.Module):\n","\n","    def __init__(self, image_net, feature_input_size, output_size):\n","        super(FeatureImageNet, self).__init__()\n","        self.image_net = image_net\n","        self.feat_fc1 = nn.Linear(feature_input_size, 46)  \n","        self.feat_fc2 = nn.Linear(46, 100)\n","        self.feat_fc3 = nn.Linear(100, 128)\n","        self.feat_fc4 = nn.Linear(128, 256)\n","\n","        self.fc1 = nn.Linear(512, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 1)\n","\n","        self.dropout = nn.Dropout(0.5, inplace=False) \n","\n","    def forward(self, images, features):\n","        x_features = F.relu(self.feat_fc1(features))\n","        x_features = F.relu(self.feat_fc2(x_features))\n","        x_features = F.relu(self.feat_fc3(x_features))\n","        x_features = F.relu(self.feat_fc4(x_features))\n","\n","        x_image = self.image_net(images)\n","      \n","        x = torch.cat([x_features, x_image] , dim=1, out=None)\n","\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc3(x))\n","\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"3JLcmaBq8EfX"},"source":["## PyTMinMaxScaler"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1151,"status":"ok","timestamp":1610576027278,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"WR2r4Qs28EfZ"},"outputs":[],"source":["class PyTMinMaxScaler():\n","    \"\"\"\n","    Transforms each channel to the range [0, 1].\n","    \"\"\"\n","    def transform(self, tensor):\n","        tensor.mul_(self.scale).sub_(self.subtract)\n","        return tensor.float() \n","    \n","    def fit(self, tensor):\n","        t = tensor.detach().clone()\n","        self.scale = 1.0 / (t.max(dim=0, keepdim=True)[0] - t.min(dim=0, keepdim=True)[0])\n","        self.scale[self.scale == float(\"Inf\")] = 0\n","        t.mul_(self.scale)\n","        self.subtract = t.min(dim=0, keepdim=True)[0]\n","        \n","    def fit_transform(self, tensor):\n","        self.scale = 1.0 / (tensor.max(dim=0, keepdim=True)[0] - tensor.min(dim=0, keepdim=True)[0])\n","        self.scale[self.scale == float(\"Inf\")] = 0\n","        tensor.mul_(self.scale)\n","        self.subtract = tensor.min(dim=0, keepdim=True)[0]\n","        tensor.sub_(self.subtract)\n","        return tensor.float() \n","        "]},{"cell_type":"markdown","metadata":{"id":"iFcM3S1o8Efa"},"source":["## Loading data"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1140,"status":"ok","timestamp":1610576027279,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"YCu9wC4J8Efb","outputId":"545c787a-8370-4ae2-c3d1-73b3a206483e"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3830807,"status":"ok","timestamp":1610580000276,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"VHAuuWy38Efd","outputId":"631d0b76-201b-4f8a-daef-4f7ccbc29aa8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading features...\n","Done\n","Loading images...\n","Done\n","Loading features...\n","Done\n","Loading images...\n","Done\n","Loading features...\n","Done\n","Loading images...\n","Done\n"]}],"source":["data_dir = \"/content/gdrive/MyDrive/biodiversity_regression\"\n","\n","image_transform = transforms.Compose([\n","        transforms.Resize((91,91)),\n","        transforms.ToTensor(),\n","        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","    ])\n","\n","feature_scaler = PyTMinMaxScaler()\n","out_scaler = PyTMinMaxScaler()\n","\n","train_dataset = BiodiversityDataset(data_dir+\"/italy_train.csv\", data_dir+\"/italyV2\", image_transform)\n","val_dataset = BiodiversityDataset(data_dir+\"/italy_validation.csv\", data_dir+\"/italyV2\", image_transform)\n","test_dataset = BiodiversityDataset(data_dir+\"/italy_test.csv\", data_dir+\"/italyV2\", image_transform)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0)\n","test_dataloader = DataLoader(test_dataset, batch_size=35, shuffle=True, num_workers=0)\n","\n","feature_scaler.fit(train_dataset.get_feature())\n","out_scaler.fit(train_dataset.get_out().reshape([-1, 1]))\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3827783,"status":"ok","timestamp":1610580000900,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"ZvVf6NgW8Eff"},"outputs":[],"source":["#resnet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\r\n","#resnet.fc = nn.Linear(512, 20)\r\n","#resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\r\n","\r\n","alexnet =  torchvision.models.alexnet(pretrained=False, progress=True)\r\n","#alexnet.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\r\n","alexnet.classifier[4] = nn.Linear(4096, 1024)\r\n","alexnet.classifier[6] = nn.Linear(1024, 256)\r\n","net = FeatureImageNet(image_net=alexnet, feature_input_size=46, output_size=1)\r\n","net.to(device)\r\n","criterion = nn.MSELoss()\r\n","optimizer = optim.Adam(net.parameters(), lr=0.0003)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3827301,"status":"ok","timestamp":1610580000902,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"bOFnhFMfwEbm","outputId":"2c0e7f33-f7f9-4f7f-e42b-99c452ba7804"},"outputs":[{"data":{"text/plain":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=1024, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Linear(in_features=1024, out_features=256, bias=True)\n","  )\n",")"]},"execution_count":18,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["alexnet"]},{"cell_type":"markdown","metadata":{"id":"Y65IxWkscewO"},"source":["## Train\r\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3825945,"status":"ok","timestamp":1610580000904,"user":{"displayName":"mino chetta","photoUrl":"","userId":"12690210461825125044"},"user_tz":-60},"id":"lT6GUejCvxXR"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"ePc8Ahf98Eff"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1892ffb8418943d694386b7c88eae4f4","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[1,   200] loss: 0.056\n","[2,   200] loss: 0.026\n","[3,   200] loss: 0.023\n","[4,   200] loss: 0.022\n","[5,   200] loss: 0.020\n","Train score at epoch 4: 0.08338610005026015\n","Validation score at epoch 4: 0.06578106332792977\n","Storing current model\n","[6,   200] loss: 0.019\n","[7,   200] loss: 0.017\n","[8,   200] loss: 0.016\n","[9,   200] loss: 0.015\n","[10,   200] loss: 0.015\n","Train score at epoch 9: 0.29527015431361436\n","Validation score at epoch 9: 0.30256543943206105\n","Storing current model\n","[11,   200] loss: 0.014\n","[12,   200] loss: 0.014\n","[13,   200] loss: 0.013\n","[14,   200] loss: 0.013\n","[15,   200] loss: 0.012\n","Train score at epoch 14: 0.3859185783351159\n","Validation score at epoch 14: 0.39032558374119297\n","Storing current model\n","[16,   200] loss: 0.012\n","[17,   200] loss: 0.011\n","[18,   200] loss: 0.011\n","[19,   200] loss: 0.011\n","[20,   200] loss: 0.011\n","Train score at epoch 19: 0.5535090428131925\n","Validation score at epoch 19: 0.5120746973022225\n","Storing current model\n","[21,   200] loss: 0.010\n","[22,   200] loss: 0.010\n","[23,   200] loss: 0.010\n","[24,   200] loss: 0.010\n","[25,   200] loss: 0.010\n","Train score at epoch 24: 0.4846801436412993\n","Validation score at epoch 24: 0.4621009905784179\n","[26,   200] loss: 0.010\n","[27,   200] loss: 0.010\n","[28,   200] loss: 0.009\n","[29,   200] loss: 0.009\n","[30,   200] loss: 0.009\n","Train score at epoch 29: 0.5507832794860106\n","Validation score at epoch 29: 0.5188065035261311\n","Storing current model\n","[31,   200] loss: 0.009\n","[32,   200] loss: 0.008\n","[33,   200] loss: 0.008\n","[34,   200] loss: 0.008\n","[35,   200] loss: 0.008\n","Train score at epoch 34: 0.6669687116060807\n","Validation score at epoch 34: 0.5420475500149462\n","Storing current model\n","[36,   200] loss: 0.007\n","[37,   200] loss: 0.007\n","[38,   200] loss: 0.007\n","[39,   200] loss: 0.006\n","[40,   200] loss: 0.006\n","Train score at epoch 39: 0.6719577492702499\n","Validation score at epoch 39: 0.42628525754078495\n","[41,   200] loss: 0.006\n","[42,   200] loss: 0.006\n","[43,   200] loss: 0.005\n","[44,   200] loss: 0.005\n","[45,   200] loss: 0.005\n","Train score at epoch 44: 0.8118641140074214\n","Validation score at epoch 44: 0.497377388339013\n","[46,   200] loss: 0.005\n","[47,   200] loss: 0.004\n","[48,   200] loss: 0.004\n","[49,   200] loss: 0.004\n","[50,   200] loss: 0.003\n","Train score at epoch 49: 0.8771772763453389\n","Validation score at epoch 49: 0.5653105363980497\n","Storing current model\n","[51,   200] loss: 0.003\n","[52,   200] loss: 0.003\n","[53,   200] loss: 0.003\n","[54,   200] loss: 0.003\n","[55,   200] loss: 0.003\n","Train score at epoch 54: 0.8836674878419185\n","Validation score at epoch 54: 0.6139141197812157\n","Storing current model\n","[56,   200] loss: 0.003\n","[57,   200] loss: 0.002\n","[58,   200] loss: 0.003\n","[59,   200] loss: 0.003\n","[60,   200] loss: 0.002\n","Train score at epoch 59: 0.916518535984543\n","Validation score at epoch 59: 0.5531758213575302\n","[61,   200] loss: 0.002\n","[62,   200] loss: 0.002\n","[63,   200] loss: 0.002\n","[64,   200] loss: 0.002\n","[65,   200] loss: 0.002\n","Train score at epoch 64: 0.8991412225144311\n","Validation score at epoch 64: 0.5443017450947893\n","[66,   200] loss: 0.002\n","[67,   200] loss: 0.002\n","[68,   200] loss: 0.002\n","[69,   200] loss: 0.002\n","[70,   200] loss: 0.002\n","Train score at epoch 69: 0.9401347224807883\n","Validation score at epoch 69: 0.5840506165319899\n","[71,   200] loss: 0.002\n","[72,   200] loss: 0.002\n","[73,   200] loss: 0.002\n","[74,   200] loss: 0.002\n","[75,   200] loss: 0.002\n","Train score at epoch 74: 0.9444902170217541\n","Validation score at epoch 74: 0.6342029164917845\n","Storing current model\n","[76,   200] loss: 0.002\n","[77,   200] loss: 0.002\n","[78,   200] loss: 0.002\n","[79,   200] loss: 0.002\n","[80,   200] loss: 0.002\n","Train score at epoch 79: 0.9454272881557841\n","Validation score at epoch 79: 0.6074524858808099\n","[81,   200] loss: 0.002\n","[82,   200] loss: 0.002\n","[83,   200] loss: 0.002\n","[84,   200] loss: 0.002\n","[85,   200] loss: 0.002\n","Train score at epoch 84: 0.9505234173441696\n","Validation score at epoch 84: 0.6065009154097909\n","[86,   200] loss: 0.002\n","[87,   200] loss: 0.001\n","[88,   200] loss: 0.002\n","[89,   200] loss: 0.002\n","[90,   200] loss: 0.002\n","Train score at epoch 89: 0.9498412283239258\n","Validation score at epoch 89: 0.5813819293836859\n","[91,   200] loss: 0.001\n","[92,   200] loss: 0.001\n","[93,   200] loss: 0.002\n","[94,   200] loss: 0.002\n","[95,   200] loss: 0.001\n","Train score at epoch 94: 0.9379456871026345\n","Validation score at epoch 94: 0.5611081399684269\n","[96,   200] loss: 0.002\n","[97,   200] loss: 0.002\n","[98,   200] loss: 0.001\n","[99,   200] loss: 0.001\n","[100,   200] loss: 0.001\n","Train score at epoch 99: 0.9521431436255727\n","Validation score at epoch 99: 0.5917489786431658\n","[101,   200] loss: 0.001\n","[102,   200] loss: 0.001\n","[103,   200] loss: 0.001\n","[104,   200] loss: 0.001\n","[105,   200] loss: 0.001\n","Train score at epoch 104: 0.952852142859652\n","Validation score at epoch 104: 0.5753183071737078\n","[106,   200] loss: 0.001\n","[107,   200] loss: 0.001\n","[108,   200] loss: 0.001\n","[109,   200] loss: 0.001\n","[110,   200] loss: 0.001\n","Train score at epoch 109: 0.9565411716327159\n","Validation score at epoch 109: 0.6258645049416163\n","[111,   200] loss: 0.001\n","[112,   200] loss: 0.001\n","[113,   200] loss: 0.001\n","[114,   200] loss: 0.001\n","[115,   200] loss: 0.001\n","Train score at epoch 114: 0.9582633848727533\n","Validation score at epoch 114: 0.6189066329149706\n","[116,   200] loss: 0.001\n","[117,   200] loss: 0.001\n","[118,   200] loss: 0.001\n","[119,   200] loss: 0.001\n","[120,   200] loss: 0.001\n","Train score at epoch 119: 0.9510154178588048\n","Validation score at epoch 119: 0.6223721015941948\n","[121,   200] loss: 0.001\n","[122,   200] loss: 0.001\n","[123,   200] loss: 0.001\n","[124,   200] loss: 0.001\n","[125,   200] loss: 0.001\n","Train score at epoch 124: 0.9514539962594899\n","Validation score at epoch 124: 0.536841169381147\n","[126,   200] loss: 0.001\n","[127,   200] loss: 0.001\n","[128,   200] loss: 0.001\n","[129,   200] loss: 0.001\n","[130,   200] loss: 0.001\n","Train score at epoch 129: 0.9562039067469476\n","Validation score at epoch 129: 0.622858857670462\n","[131,   200] loss: 0.001\n","[132,   200] loss: 0.001\n","[133,   200] loss: 0.001\n","[134,   200] loss: 0.001\n","[135,   200] loss: 0.001\n","Train score at epoch 134: 0.9482977976594793\n","Validation score at epoch 134: 0.562067369422696\n","[136,   200] loss: 0.001\n","[137,   200] loss: 0.001\n","[138,   200] loss: 0.001\n","[139,   200] loss: 0.001\n","[140,   200] loss: 0.001\n","Train score at epoch 139: 0.9608467875499853\n","Validation score at epoch 139: 0.6042534906438892\n","[141,   200] loss: 0.001\n","[142,   200] loss: 0.001\n","[143,   200] loss: 0.001\n","[144,   200] loss: 0.001\n","[145,   200] loss: 0.001\n","Train score at epoch 144: 0.9624429108155588\n","Validation score at epoch 144: 0.6015890639278052\n","[146,   200] loss: 0.001\n","[147,   200] loss: 0.001\n","[148,   200] loss: 0.001\n","[149,   200] loss: 0.001\n","[150,   200] loss: 0.001\n","Train score at epoch 149: 0.9621127965051441\n","Validation score at epoch 149: 0.6146445211959337\n","[151,   200] loss: 0.001\n","[152,   200] loss: 0.001\n","[153,   200] loss: 0.001\n","[154,   200] loss: 0.001\n","[155,   200] loss: 0.001\n","Train score at epoch 154: 0.9565390182890745\n","Validation score at epoch 154: 0.5830085002448417\n","[156,   200] loss: 0.001\n","[157,   200] loss: 0.001\n","[158,   200] loss: 0.001\n","[159,   200] loss: 0.001\n","[160,   200] loss: 0.001\n","Train score at epoch 159: 0.9601573436069889\n","Validation score at epoch 159: 0.5904075510451878\n","[161,   200] loss: 0.001\n","[162,   200] loss: 0.001\n","[163,   200] loss: 0.001\n","[164,   200] loss: 0.001\n","[165,   200] loss: 0.001\n","Train score at epoch 164: 0.9580040173302242\n","Validation score at epoch 164: 0.6242380124821241\n","[166,   200] loss: 0.001\n","[167,   200] loss: 0.001\n","[168,   200] loss: 0.001\n","[169,   200] loss: 0.001\n","[170,   200] loss: 0.001\n","Train score at epoch 169: 0.9376301239714061\n","Validation score at epoch 169: 0.4929114698111773\n","[171,   200] loss: 0.001\n","[172,   200] loss: 0.001\n","[173,   200] loss: 0.001\n","[174,   200] loss: 0.001\n","[175,   200] loss: 0.001\n","Train score at epoch 174: 0.9651701767421591\n","Validation score at epoch 174: 0.6215059572149295\n","[176,   200] loss: 0.001\n","[177,   200] loss: 0.001\n","[178,   200] loss: 0.001\n","[179,   200] loss: 0.001\n","[180,   200] loss: 0.001\n","Train score at epoch 179: 0.9604055965253666\n","Validation score at epoch 179: 0.5795089168037126\n","[181,   200] loss: 0.001\n","[182,   200] loss: 0.001\n","[183,   200] loss: 0.001\n","[184,   200] loss: 0.001\n","[185,   200] loss: 0.001\n","Train score at epoch 184: 0.9591798836244392\n","Validation score at epoch 184: 0.5916189487348771\n","[186,   200] loss: 0.001\n","[187,   200] loss: 0.001\n","[188,   200] loss: 0.001\n","[189,   200] loss: 0.001\n","[190,   200] loss: 0.001\n","Train score at epoch 189: 0.9629375798684737\n","Validation score at epoch 189: 0.56954041189093\n","[191,   200] loss: 0.001\n","[192,   200] loss: 0.001\n","[193,   200] loss: 0.001\n","[194,   200] loss: 0.001\n","[195,   200] loss: 0.001\n","Train score at epoch 194: 0.9653526811618625\n","Validation score at epoch 194: 0.5900281765410336\n","[196,   200] loss: 0.001\n","[197,   200] loss: 0.001\n","[198,   200] loss: 0.001\n","[199,   200] loss: 0.001\n","[200,   200] loss: 0.001\n","Train score at epoch 199: 0.9647130308850422\n","Validation score at epoch 199: 0.5601488103690455\n","\n"]}],"source":["model, best_model, score = train_model(train_dataloader=train_dataloader, val_dataloader=val_dataloader, \n","                        model=net, optimizer=optimizer, out_scaler=out_scaler, feature_scaler=feature_scaler,\n","                        criterion=criterion, epochs=200, print_step=200, val_step=5)\n","torch.save(best_model, data_dir + \"/best_model_italy.pt\")"]},{"cell_type":"markdown","metadata":{"id":"EMMFXUMcU9w6"},"source":["## Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pmbJ7eoA8Efg"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best validation model test score: 0.6522562417973514\n","Full trained Model test score: 0.5950923385047264\n"]}],"source":["#best_model = torch.load(data_dir + \"/best_model_italy.pt\")\r\n","best_model_score = test_model(test_dataloader, best_model)\r\n","print(f\"Best validation model test score: {best_model_score}\")\r\n","model_score = test_model(test_dataloader, model)\r\n","print(f\"Full trained Model test score: {model_score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"52hxkw3s0iVQ"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"regression_image.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0d7898ff2c784792b078431fc2ff76d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1892ffb8418943d694386b7c88eae4f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75996c17bf754fd58e6861a81f1be2ad","IPY_MODEL_a64daa3a4e4047139d2d0919b64d7193"],"layout":"IPY_MODEL_f9d29f938b6c477f9f99c4f7f0d1fd8d"}},"52850808b2914d818532fcf29b1396d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"75996c17bf754fd58e6861a81f1be2ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":" 68%","description_tooltip":null,"layout":"IPY_MODEL_97ce34fa8aad45df816ea35d857e715a","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52850808b2914d818532fcf29b1396d3","value":136}},"97ce34fa8aad45df816ea35d857e715a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a64daa3a4e4047139d2d0919b64d7193":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc5b05eaf94f449aaf5ca786e666533a","placeholder":"​","style":"IPY_MODEL_0d7898ff2c784792b078431fc2ff76d1","value":" 136/200 [44:09\u0026lt;21:42, 20.36s/it]"}},"dc5b05eaf94f449aaf5ca786e666533a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9d29f938b6c477f9f99c4f7f0d1fd8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression using satellite images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, optimizer, criterion,  out_scaler, feature_scaler, epochs=20, print_step=1000):\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, sample_batched in enumerate(dataloader):\n",
    "            \n",
    "            expected_outputs = out_scaler.transform(sample_batched['out'])\n",
    "            features = feature_scaler.transform(sample_batched['features'])\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(sample_batched['image'].to(device), features.to(device))\n",
    "            loss = criterion(outputs, expected_outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_step == print_step-1:    # print every 500 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / print_step))\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        expected_outputs = []\n",
    "        outputs = []\n",
    "        for sample_batched in iter(dataloader):\n",
    "            expected_outputs += out_scaler.transform(sample_batched['out'])\n",
    "            features = feature_scaler.transform(sample_batched['features'])\n",
    "\n",
    "            outputs += net(sample_batched['image'].to(device), features.to(device))\n",
    "    print(expected_outputs)\n",
    "    print(outputs)\n",
    "    print(f\"R2 score: {r2_score(outputs, expected_outputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiodiversityDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiodiversityDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, img_dir, image_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with features.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a image.\n",
    "        \"\"\"\n",
    "        self.features = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lon = self.features.loc[idx].longitude\n",
    "        lat = self.features.loc[idx].latitude\n",
    "        img_name = str(lon) + \"_\" + str(lat) + \".jpg\"\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        out = torch.from_numpy(np.array([self.features.loc[idx]['habitat_richness']])).detach().clone().reshape([-1])\n",
    "        features = self.features.drop(columns=['habitat_richness', 'longitude', 'latitude']).loc[idx].values\n",
    "        features = torch.from_numpy(features.astype('float').reshape(-1, 46)).detach().clone().reshape([-1])\n",
    "        sample = {'image': image, 'features': features, 'out': out}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def get_feature(self):\n",
    "         return torch.from_numpy(self.features.drop(columns=['habitat_richness', 'longitude', 'latitude']).values).detach().clone()\n",
    "    \n",
    "    def get_out(self):\n",
    "        return torch.from_numpy(self.features['habitat_richness'].values).detach().clone()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImageNet(nn.Module):\n",
    "\n",
    "    def __init__(self, image_net, feature_input_size, output_size):\n",
    "        super(FeatureImageNet, self).__init__()\n",
    "        self.image_net = image_net\n",
    "    \n",
    "        self.feat_fc1 = nn.Linear(feature_input_size, 46)  \n",
    "        self.feat_fc2 = nn.Linear(46, 100)\n",
    "        self.feat_fc3 = nn.Linear(100, 128)\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, images, features):\n",
    "        x_features = F.relu(self.feat_fc1(features))\n",
    "        x_features = F.relu(self.feat_fc2(x_features))\n",
    "        x_features = F.relu(self.feat_fc3(x_features))\n",
    "        \n",
    "        x_image = self.image_net(images)\n",
    "      \n",
    "        x = torch.cat([x_features, x_image] , dim=1, out=None)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTMinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTMinMaxScaler():\n",
    "    \"\"\"\n",
    "    Transforms each channel to the range [0, 1].\n",
    "    \"\"\"\n",
    "    def transform(self, tensor):\n",
    "        tensor.mul_(self.scale).sub_(self.subtract)\n",
    "        return tensor.float() \n",
    "    \n",
    "    def fit(self, tensor):\n",
    "        t = tensor.detach().clone()\n",
    "        self.scale = 1.0 / (t.max(dim=0, keepdim=True)[0] - t.min(dim=0, keepdim=True)[0])\n",
    "        self.scale[self.scale == float(\"Inf\")] = 0\n",
    "        t.mul_(self.scale)\n",
    "        self.subtract = t.min(dim=0, keepdim=True)[0]\n",
    "        \n",
    "    def fit_transform(self, tensor):\n",
    "        self.scale = 1.0 / (tensor.max(dim=0, keepdim=True)[0] - tensor.min(dim=0, keepdim=True)[0])\n",
    "        self.scale[self.scale == float(\"Inf\")] = 0\n",
    "        tensor.mul_(self.scale)\n",
    "        self.subtract = tensor.min(dim=0, keepdim=True)[0]\n",
    "        tensor.sub_(self.subtract)\n",
    "        return tensor.float() \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\minoc/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../Dataset/images\"\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "feature_scaler = PyTMinMaxScaler()\n",
    "out_scaler = PyTMinMaxScaler()\n",
    "\n",
    "dataset = BiodiversityDataset(data_dir+\"/test.csv\", data_dir+\"/test\", image_transform)\n",
    "\n",
    "feature_scaler.fit(dataset.get_feature())\n",
    "out_scaler.fit(dataset.get_out().reshape([-1, 1]))\n",
    "\n",
    "\n",
    "resnet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "resnet.fc = nn.Linear(512, 128)\n",
    "\n",
    "net = FeatureImageNet(image_net=resnet, feature_input_size=46, output_size=1)\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     2] loss: 0.039\n",
      "[2,     2] loss: 0.016\n",
      "[3,     2] loss: 0.254\n",
      "[4,     2] loss: 0.035\n",
      "[5,     2] loss: 0.236\n",
      "[6,     2] loss: 0.157\n",
      "[7,     2] loss: 0.103\n",
      "[8,     2] loss: 0.197\n",
      "[9,     2] loss: 0.279\n",
      "[10,     2] loss: 0.196\n",
      "[11,     2] loss: 0.195\n",
      "[12,     2] loss: 0.147\n",
      "[13,     2] loss: 0.076\n",
      "[14,     2] loss: 0.209\n",
      "[15,     2] loss: 0.059\n",
      "[16,     2] loss: 0.136\n",
      "[17,     2] loss: 0.049\n",
      "[18,     2] loss: 0.061\n",
      "[19,     2] loss: 0.183\n",
      "[20,     2] loss: 0.033\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_model(dataloader=dataloader, model=net, optimizer=optimizer,  out_scaler=out_scaler, \n",
    "            feature_scaler=feature_scaler, criterion=criterion, epochs=20, print_step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.9366]), tensor([0.9366]), tensor([1.]), tensor([0.]), tensor([1.]), tensor([0.]), tensor([0.4845])]\n",
      "[tensor([0.8534]), tensor([0.3932]), tensor([1.0400]), tensor([0.0380]), tensor([0.7545]), tensor([0.1191]), tensor([0.8299])]\n",
      "R2 score: 0.45784441850545543\n"
     ]
    }
   ],
   "source": [
    "test_model(dataloader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> 1402ea34c3d53b8a8227b78c174d5c64c48795f2
